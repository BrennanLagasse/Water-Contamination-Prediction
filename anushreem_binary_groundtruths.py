# -*- coding: utf-8 -*-
"""anushreem_binary_groundtruths.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gDyCBKgq5N5M3N1FzKMrNgWPG9ASnRuV
"""

import pandas as pd
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from torch.utils.data import Dataset
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.optim import Adam
from sklearn.metrics import confusion_matrix

from google.colab import files
uploaded = files.upload()
df = pd.read_csv("William - WQP Arsenic Ground Truths (CA only, Used for training).csv", index_col=False)

df.head()

#adding the 1/0 column
df.reset_index(drop=True, inplace=True)
df['Is_Detected'] = (df['Result_Measure'] > 10).astype(int)
print(df.head())
print(df.columns)

x = df.drop(columns='Is_Detected')
y = df['Is_Detected']

x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.33, random_state=42, stratify=y)
x_train_split, x_val, y_train_split, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42, stratify=y_train)

x_train_split = x_train_split.to_numpy()
x_val = x_val.to_numpy()
x_test = x_test.to_numpy()
y_train_split = y_train_split.to_numpy()
y_val = y_val.to_numpy()
y_test = y_test.to_numpy()

scaler = StandardScaler()
x_train_split = scaler.fit_transform(x_train_split)
x_val = scaler.transform(x_val)
x_test = scaler.transform(x_test)

class Train(Dataset):
  def __init__(self,x_data, y_data):
    self.x_data = x_data
    self.y_data= y_data
  def __getitem__(self, index):
    return self.x_data[index], self.y_data[index]
  def __len__(self):
    return len(self.x_data)
# train_data = Train(torch.FloatTensor(x_train), torch.FloatTensor(y_train))

class Test(Dataset):
  def __init__(self,x_data):
    self.x_data = x_data
  def __getitem__(self, index):
    return self.x_data[index]
  def __len__(self):
    return len(self.x_data)
test_data = Test(torch.FloatTensor(x_test))

HIDDEN_SIZE = 32
EPOCHS = 10
BATCH_SIZE = 32
LEARNING_RATE = 0.001
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

train_data = Train(torch.FloatTensor(x_train_split), torch.FloatTensor(y_train_split))
train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(dataset=test_data, batch_size=1)

val_data = Train(torch.FloatTensor(x_val), torch.FloatTensor(y_val))
val_loader = DataLoader(dataset=val_data, batch_size=BATCH_SIZE)

class RCAModel(nn.Module):
  def __init__(self,input_size,hidden_size):
    super(RCAModel,self).__init__()
    self.l1 = nn.Linear(input_size, hidden_size)
    self.relu1=nn.ReLU()
    self.batchnorm1 = nn.BatchNorm1d(hidden_size)
    self.l2 = nn.Linear(hidden_size, hidden_size)
    self.relu2 = nn.ReLU()
    self.batchnorm2 = nn.BatchNorm1d(hidden_size)
    self.dropout = nn.Dropout(0.5)
    self.out = nn.Linear(hidden_size, 1)
  def forward(self,x):
    x = self.l1(x)
    x = self.relu1(x)
    x= self.batchnorm1(x)
    x=self.l2(x)
    x = self.relu2(x)
    x =self.batchnorm2(x)
    x = self.dropout(x)
    x=self.out(x)
    return x

model = RCAModel(input_size=x_train_split.shape[1], hidden_size=HIDDEN_SIZE)
model.to(device)
criterion = nn.BCEWithLogitsLoss()
# optimizer = Adam(model.parameters(), lr=LEARNING_RATE)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-3) # adding weight decay i.e. l2 normalization
model

def binary_acc(y_pred, y_test):
  y_pred_tag = torch.round(torch.sigmoid(y_pred))
  correct = (y_pred_tag == y_test).float()
  acc = (correct.sum()/len(correct))*100
  return acc

model.train()
for e in range(1,EPOCHS+1):
  epoch_loss = 0
  epoch_acc = 0
  for x_batch, y_batch in train_loader:
    x_batch, y_batch = x_batch.to(device), y_batch.to(device)
    optimizer.zero_grad()
    y_pred = model(x_batch)
    loss = criterion(y_pred, y_batch.unsqueeze(1))
    acc = binary_acc(y_pred, y_batch.unsqueeze(1))
    loss.backward()
    optimizer.step()
    epoch_loss+=loss.item()
    epoch_acc+=acc.item()
  print(f"Epoch {e:02} | Loss: {epoch_loss:.4f} | Acc: {epoch_acc / len(train_loader):.2f}%")

from sklearn.metrics import accuracy_score, confusion_matrix

def eval_split(model, loader, y_true, device, split_name="SPLIT"):
    model.eval()
    y_pred_list = []
    with torch.no_grad():
        for batch in loader:
            # Handle loaders that yield (x, y) vs. just x
            if isinstance(batch, (list, tuple)):
                xb = batch[0]
            else:
                xb = batch
            xb = xb.to(device)

            logits = model(xb)
            probs = torch.sigmoid(logits)
            preds = torch.round(probs)  # threshold 0.5

            # Move to CPU numpy
            y_pred_list.append(preds.cpu().numpy())

    # Flatten to a simple 1D list
    y_pred_list = [a.squeeze().tolist() for a in y_pred_list]

    acc = accuracy_score(y_true, y_pred_list)
    cm = confusion_matrix(y_true, y_pred_list)
    print(f"[{split_name}] Accuracy: {acc*100:.2f}%")
    print(f"[{split_name}] Confusion matrix:\n{cm}\n")
    return acc, cm


train_acc, train_cm = eval_split(model,
    DataLoader(Train(torch.FloatTensor(x_train_split), torch.FloatTensor(y_train_split)), batch_size=1),
    y_train_split, device, split_name="TRAIN")

val_acc, val_cm = eval_split(model,
    DataLoader(Train(torch.FloatTensor(x_val), torch.FloatTensor(y_val)), batch_size=1),
    y_val, device, split_name="VAL")

test_acc, test_cm = eval_split(model,
    DataLoader(Test(torch.FloatTensor(x_test)), batch_size=1),
    y_test, device, split_name="TEST")