# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JI9Z7fV7BOYKAQzK5h7nmtoodXvLGc_z
"""

import pandas as pd
import numpy as np
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)

# WQP stations and samples loading
samples = pd.read_csv('resultphyschem.csv',index_col=0)
station = pd.read_csv('station.csv',index_col=0)

# merging the WQP samples and WQP stations, they shared MonitoringLocationIdentifier

joineddata = samples.merge(station, how = 'left', on = 'MonitoringLocationIdentifier')
print(joineddata.columns)
columns_to_keep = [
    'MonitoringLocationIdentifier',
    'HUCEightDigitCode',
    'LatitudeMeasure',
    'LongitudeMeasure',
    'CharacteristicName',
    'ResultMeasure/MeasureUnitCode',
    'ResultMeasureValue',
]


joineddatanew = joineddata[columns_to_keep] #keeping important cols
joineddatanew['ResultMeasureValue'] = pd.to_numeric(joineddatanew['ResultMeasureValue'], errors='coerce')
joineddata['DetectionQuantitationLimitMeasure/MeasureValue'] = pd.to_numeric(joineddata['DetectionQuantitationLimitMeasure/MeasureValue'], errors='coerce')

# imputing missing measures using half of detectionLimit (reasonable estimate)
mask_A = joineddatanew['ResultMeasureValue'].isna() & joineddata['DetectionQuantitationLimitMeasure/MeasureValue'].notna()
joineddatanew.loc[mask_A, 'ResultMeasureValue'] = joineddata.loc[mask_A, 'DetectionQuantitationLimitMeasure/MeasureValue'] / 2
joineddatanew.loc[mask_A, "ResultMeasure/MeasureUnitCode"] = joineddata.loc[mask_A, "DetectionQuantitationLimitMeasure/MeasureUnitCode"]

# Converting all values to ug/L
mask_mg = (joineddatanew["ResultMeasure/MeasureUnitCode"] == "mg/L") | (joineddatanew["ResultMeasure/MeasureUnitCode"] == "mg/kg")
joineddatanew.loc[mask_mg, "ResultMeasureValue"] *= 1000
joineddatanew = joineddatanew.drop("ResultMeasure/MeasureUnitCode", axis=1)

# Dropping all NA Values
joineddatanew = joineddatanew.dropna(subset=[
    "LatitudeMeasure",
    "LongitudeMeasure",
    "ResultMeasureValue",
    "HUCEightDigitCode"
])

# separating the dataframe by arsenic and boron values
joineddatayes = joineddatanew[joineddatanew["CharacteristicName"] == "Arsenic"]
joineddatanew = joineddatanew[joineddatanew['CharacteristicName']=='Boron']
joineddatanew.drop(columns=['CharacteristicName']) #dropping characteristic names
cols = ['LatitudeMeasure','LongitudeMeasure',"ResultMeasureValue"]
ground_truth = joineddatayes[cols] # keeping lat/long and arsenic values


joineddatanew.rename(columns ={'ResultMeasureValue':'BoronValue'}, inplace = True) # Renamed in Boron dataframe for readability

ground_truth.shape

usgs = pd.read_csv('fullphyschem (1).csv', index_col=0) # USGS dataset with pfa values
cols_keep = ['Location_Identifier', 'Location_HUCEightDigitCode','Location_HUCTwelveDigitCode','Location_Latitude','Location_Longitude','Result_ResultDetectionCondition','Result_Measure','Result_MeasureUnit','DetectionLimit_MeasureA','DetectionLimit_MeasureB','DetectionLimit_MeasureUnitA','DetectionLimit_MeasureUnitB']
usgs = usgs[cols_keep] # keeping important cols

# imputing missing pfa values using same process as above (detectionLimitA)
maskA1 = usgs['Result_Measure'].isna() & usgs['DetectionLimit_MeasureA'].notna()
usgs.loc[maskA1, 'Result_Measure'] = usgs.loc[maskA1, 'DetectionLimit_MeasureA'] / 2
usgs.loc[maskA1, "Result_MeasureUnit"] = usgs.loc[maskA1, "DetectionLimit_MeasureUnitA"]

# same as above but for values with only DetectionLimitB
maskB1 = usgs['Result_Measure'].isna() & usgs['DetectionLimit_MeasureA'].isna() & usgs['DetectionLimit_MeasureB'].notna()
usgs.loc[maskB1, 'Result_Measure'] = usgs.loc[maskB1, 'DetectionLimit_MeasureB'] / 2
usgs.loc[maskB1, "Result_MeasureUnit"] = usgs.loc[maskB1, "DetectionLimit_MeasureUnitB"]

usgs['Is_Detected'] = usgs['Result_Measure'].notna().astype(int)

# Converting all values to ug/L
mask_mg = (usgs["Result_MeasureUnit"] == "ng/L")
usgs.loc[mask_mg, "Result_Measure"] /= 1000
usgs = usgs.drop("Result_MeasureUnit", axis=1)

# Dropping all NA Values
usgs = usgs.dropna(subset=[
    "Location_Latitude",
    "Location_Longitude",
    "Result_Measure",
    "Location_HUCEightDigitCode"
])



usgs.to_csv('usgs.csv',index=False) # pfa dataset cleaned

# renaming columns for readability
usgs.rename(columns ={'Location_HUCEightDigitCode':'HUCEightDigitCode'}, inplace = True)
usgs.rename(columns ={'Location_Latitude':'LatitudeMeasure'}, inplace = True)
usgs.rename(columns ={'Location_Longitude':'LongitudeMeasure'}, inplace = True)
usgs.rename(columns ={'Location_Identifier':'MonitoringLocationIdentifier'}, inplace = True)


# Round lat/long in both DataFrames, in case they have variance of negligible values and don't merge
usgs.loc[:, 'LatitudeMeasure'] = usgs.loc[:, 'LatitudeMeasure'].round(5)
usgs.loc[:, 'LongitudeMeasure'] = usgs.loc[:, 'LongitudeMeasure'].round(5)

joineddatanew.loc[:, 'LatitudeMeasure'] = joineddatanew.loc[:, 'LatitudeMeasure'].round(5)
joineddatanew.loc[:, 'LongitudeMeasure'] = joineddatanew.loc[:, 'LongitudeMeasure'].round(5)

ground_truth.loc[:, 'LatitudeMeasure'] = ground_truth.loc[:, 'LatitudeMeasure'].round(5)
ground_truth.loc[:, 'LongitudeMeasure'] = ground_truth.loc[:, 'LongitudeMeasure'].round(5)


# combined the WQP arsenic, WQP Boron, and USGS pfa values
combined = usgs.merge(joineddatanew, on = ['LatitudeMeasure','LongitudeMeasure'], how = 'inner', suffixes = ['_bo','_pfa'])
combined = combined.drop(columns=['CharacteristicName','Result_ResultDetectionCondition','DetectionLimit_MeasureA','DetectionLimit_MeasureB','DetectionLimit_MeasureUnitA','DetectionLimit_MeasureUnitB','Is_Detected','Location_HUCTwelveDigitCode','HUCEightDigitCode_bo','HUCEightDigitCode_pfa','MonitoringLocationIdentifier_bo','MonitoringLocationIdentifier_pfa'])
combined2 = ground_truth.merge(combined, on = ['LatitudeMeasure','LongitudeMeasure'], how = 'inner')
combined2.dropna(subset = ['Result_Measure','BoronValue','ResultMeasureValue'])
combined2.shape

# william's soil data loaded
soil = pd.read_csv('soildata.csv')

# renamed columns for merging
soil.rename(columns ={'Location_LongitudeStandardized':'LongitudeMeasure'}, inplace = True)
soil.rename(columns ={'Location_LatitudeStandardized':'LatitudeMeasure'}, inplace = True)
soil.columns

# rounded soil lat/long for merging
soil.loc[:, 'LatitudeMeasure'] = soil.loc[:, 'LatitudeMeasure'].round(5)
soil.loc[:, 'LongitudeMeasure'] = soil.loc[:, 'LongitudeMeasure'].round(5)


# ---- KNN joining of soil and arsenic, boron, pfa values---

from sklearn.neighbors import NearestNeighbors

#  Convert coordinates to radians (for haversine)
soil_coords_rad = np.radians(soil[['LatitudeMeasure', 'LongitudeMeasure']].to_numpy())
chem_coords_rad = np.radians(combined2[['LatitudeMeasure', 'LongitudeMeasure']].to_numpy())

#  Use NearestNeighbors with haversine distance
nn = NearestNeighbors(n_neighbors=1, metric='haversine')
nn.fit(chem_coords_rad)

#  Find nearest chem site for each soil site
distances_rad, indices = nn.kneighbors(soil_coords_rad)
distances_m = distances_rad.flatten() * 6371000  # Convert to meters

# Extract matching rows from chem_df
matched_chem_df = combined2.iloc[indices.flatten()].reset_index(drop=True)
matched_chem_df = matched_chem_df.add_prefix('chem_')

# Add distance to soil_df and combine
merged_df = pd.concat([soil.reset_index(drop=True), matched_chem_df], axis=1)
print(merged_df.columns)

merged_df.drop(columns=['Location_HUCEightDigitCode','Is_Detected','Result_Characteristic_Arsenic','Result_Characteristic_Arsenic ion (3+)','Result_Characteristic_Arsenic, Inorganic','year','month','day', 'chem_LatitudeMeasure', 'chem_LongitudeMeasure',],inplace = True)
merged_df.columns

# chemresultmeasure value = arsenic values
test = merged_df['chem_ResultMeasureValue'] # getting arsenic data isolated
merged_df.drop(columns=['chem_ResultMeasureValue'], inplace = True)


# get train and output csvs
test.to_csv('arsenics.csv',index=False)
merged_df.to_csv('trainingfinal.csv',index=False)

print(test.shape)
print(merged_df.shape)
print(merged_df.head())